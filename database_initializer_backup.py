"""
Script unificado de inicializaci√≥n de base de datos para entornos locales y externos (Railway).
Este script configura autom√°ticamente toda la estructura de base de datos
y carga los datos necesarios para el API Dashboard de la Alcald√≠a de Cali.

Funcionalidades:
- Detecta autom√°ticamente si est√° en entorno local o Railway
- Crea estructura de base de datos completa
- Carga datos desde archivos JSON/CSV disponibles
- Maneja variables de entorno (.env para Railway)
- Verifica integridad de datos cargados
"""

import logging
import sys
import os
import json
from pathlib import Path
from sqlalchemy import create_engine, text, inspect
from sqlalchemy.exc import OperationalError, ProgrammingError
from typing import Dict, List, Tuple, Optional
import time
import datetime
from dotenv import load_dotenv
from tqdm import tqdm
import psutil
import hashlib

# Cargar variables de entorno
load_dotenv()

# Agregar el directorio ra√≠z al path para importar m√≥dulos
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from fastapi_project.database import SQLALCHEMY_DATABASE_URL, engine
from fastapi_project import models

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('database_init.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DatabaseInitializer:
    """Inicializa y configura la base de datos completa para entornos locales y externos.

    Responsabilidades:
    - Detectar entorno (local/Railway) autom√°ticamente
    - Verificar conexi√≥n y configuraci√≥n
    - Crear estructura de base de datos
    - Cargar datos incrementalmente (solo nuevos datos)
    - Generar reportes detallados de m√©tricas
    """
    
    def __init__(self):
        """Inicializa el configurador de base de datos."""
        self.engine = engine
        self.start_time = time.time()
        self.metrics = {
            'start_time': datetime.datetime.now(),
            'environment': self._detect_environment(),
            'tables_created': 0,
            'tables_updated': 0,
            'data_loaded': {},
            'data_skipped': {},
            'errors': [],
            'performance': {},
            'memory_usage': {},
            'files_processed': 0,
            'total_records': 0,
            'failed_records': 0
        }
        logger.info(f"üåç Entorno detectado: {self.metrics['environment']}")
        
    def _detect_environment(self) -> str:
        """Detecta si est√° ejecut√°ndose en Railway o entorno local."""
        if os.getenv('RAILWAY_ENVIRONMENT'):
            return 'Railway (Producci√≥n)'
        elif os.getenv('DATABASE_URL') and 'railway' in os.getenv('DATABASE_URL', ''):
            return 'Railway (Desarrollo)'
        return 'Local (Desarrollo)'
        
    def _get_memory_usage(self) -> Dict[str, float]:
        """Obtiene el uso actual de memoria."""
        process = psutil.Process()
        memory_info = process.memory_info()
        return {
            'rss_mb': memory_info.rss / 1024 / 1024,  # Memoria f√≠sica
            'vms_mb': memory_info.vms / 1024 / 1024,  # Memoria virtual
            'percent': process.memory_percent()
        }

    def _get_file_hash(self, file_path: Path) -> str:
        """Calcula hash MD5 de un archivo para detectar cambios."""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
        
    def _check_table_exists_and_has_data(self, table_name: str) -> Tuple[bool, int]:
        """Verifica si una tabla existe y cu√°ntos registros tiene."""
        try:
            with self.engine.connect() as conn:
                # Verificar si la tabla existe
                if not self.engine.dialect.has_table(conn, table_name):
                    return False, 0
                
                # Contar registros
                result = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
                count = result.scalar()
                return True, count
        except Exception as e:
            logger.warning(f"Error verificando tabla {table_name}: {e}")
            return False, 0
            
    def _should_load_data(self, table_name: str, file_path: Path) -> bool:
        """Determina si se deben cargar datos bas√°ndose en existencia y cambios."""
        exists, count = self._check_table_exists_and_has_data(table_name)
        
        if not exists:
            logger.info(f"üì• {table_name}: Tabla no existe, se cargar√°")
            return True
            
        if count == 0:
            logger.info(f"üì• {table_name}: Tabla vac√≠a, se cargar√°")
            return True
            
        # TODO: Implementar verificaci√≥n de hash de archivo para detectar cambios
        # Por ahora, no cargar si ya tiene datos
        logger.info(f"‚è≠Ô∏è {table_name}: Ya tiene {count:,} registros, se omite")
        self.metrics['data_skipped'][table_name] = count
        return False
        
    def test_connection(self) -> bool:
        """Verifica la conexi√≥n a la base de datos mediante un SELECT 1."""
        try:
            with self.engine.connect() as connection:
                connection.execute(text("SELECT 1"))
            logger.info("‚úÖ Conexi√≥n a la base de datos exitosa")
            return True
        except OperationalError as e:
            logger.error(f"‚ùå Error de conexi√≥n a la base de datos: {e}")
            return False
        except Exception as e:
            logger.error(f"‚ùå Error inesperado de conexi√≥n: {e}")
            return False
    
    def get_existing_tables(self) -> List[str]:
        """Obtiene la lista de tablas existentes en la base de datos."""
        try:
            tables = self.inspector.get_table_names()
            logger.info(f"üìä Tablas existentes encontradas: {len(tables)}")
            return tables
        except Exception as e:
            logger.error(f"‚ùå Error al obtener tablas existentes: {e}")
            return []
    
    def backup_existing_data(self, tables: List[str]) -> Dict[str, int]:
        """Cuenta registros por tabla importante para fines de backup r√°pido (metadatos)."""
        backup_counts = {}
        important_tables = [
            'centros_gestores', 'programas', 'areas_funcionales', 
            'propositos', 'retos', 'movimientos_presupuestales', 
            'ejecucion_presupuestal', 'datos_caracteristicos_proyectos',  # Nueva tabla
            'unidades_proyecto_infraestructura_equipamientos',
            'unidades_proyecto_infraestructura_vial', 'seguimiento_pa',
            'seguimiento_productos_pa', 'seguimiento_actividades_pa'
        ]
        
        try:
            with self.engine.connect() as connection:
                for table in important_tables:
                    if table in tables:
                        result = connection.execute(text(f"SELECT COUNT(*) FROM {table}"))
                        count = result.scalar()
                        backup_counts[table] = count
                        logger.info(f"üìã {table}: {count} registros existentes")
        except Exception as e:
            logger.error(f"‚ùå Error al hacer backup de datos: {e}")
        
        return backup_counts
    
    def create_tables_with_correct_schema(self):
        """Crea/actualiza tablas usando SQLAlchemy modelos con esquema corregido."""
        logger.info("üîß Creando/actualizando estructura de tablas desde modelos SQLAlchemy...")
        
        try:
            # Obtener tablas existentes antes
            existing_tables = self.get_existing_tables()
            
            # Para Railway, aseguramos que las tablas problem√°ticas se actualicen
            if self.is_railway_environment():
                self._update_problematic_tables()
            
            # Usar SQLAlchemy para crear todas las tablas definidas en models.py
            models.Base.metadata.create_all(bind=self.engine)
            logger.info("‚úÖ Todas las tablas creadas/verificadas desde modelos SQLAlchemy")
            
            # Listar las tablas creadas
            created_tables = self.get_existing_tables()
            logger.info(f"üìä Tablas existentes encontradas: {len(created_tables)}")
            logger.info(f"üìä Tablas disponibles ({len(created_tables)}):")
            for table in sorted(created_tables):
                logger.info(f"   ‚Ä¢ {table}")
            
            # Crear √≠ndices importantes para rendimiento
            self.create_performance_indexes()
                    
        except Exception as e:
            logger.error(f"‚ùå Error creando tablas: {e}")
            raise

    def _update_problematic_tables(self):
        """Actualiza tablas que tienen diferencias de esquema en Railway."""
        problematic_tables = [
            'movimientos_presupuestales',
            'unidades_proyecto_infraestructura_equipamientos', 
            'unidades_proyecto_infraestructura_vial'
        ]
        
        for table_name in problematic_tables:
            try:
                # Agregar columnas faltantes si no existen
                if table_name == 'movimientos_presupuestales':
                    self._add_columns_if_missing(table_name, [
                        ('aplazamiento', 'BIGINT DEFAULT 0'),
                        ('desaplazamiento', 'BIGINT DEFAULT 0')
                    ])
                elif 'infraestructura' in table_name:
                    self._add_columns_if_missing(table_name, [
                        ('avance_f√≠sico_obra', 'FLOAT'),
                        ('ejecucion_financiera_obra', 'FLOAT')
                    ])
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è No se pudo actualizar {table_name}: {e}")

    def _add_columns_if_missing(self, table_name: str, columns: list):
        """Agrega columnas a una tabla si no existen."""
        with self.engine.connect() as conn:
            for column_name, column_type in columns:
                try:
                    # Verificar si la columna existe
                    result = conn.execute(text(f"""
                        SELECT column_name 
                        FROM information_schema.columns 
                        WHERE table_name = '{table_name}' 
                        AND column_name = '{column_name}'
                    """))
                    
                    if not result.fetchone():
                        # La columna no existe, agregarla
                        conn.execute(text(f"ALTER TABLE {table_name} ADD COLUMN {column_name} {column_type}"))
                        conn.commit()
                        logger.info(f"‚úÖ Columna {column_name} agregada a {table_name}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è No se pudo agregar columna {column_name} a {table_name}: {e}")
                    pass
    
    def create_performance_indexes(self):
        """Crea √≠ndices importantes para mejorar el rendimiento de consultas con barra de progreso."""
        logger.info("üîß Creando √≠ndices de rendimiento...")
        
        indices = [
            # √çndices para movimientos presupuestales
            "CREATE INDEX IF NOT EXISTS idx_movimientos_bpin ON movimientos_presupuestales(bpin)",
            "CREATE INDEX IF NOT EXISTS idx_movimientos_periodo ON movimientos_presupuestales(periodo_corte)",
            
            # √çndices para ejecuci√≥n presupuestal
            "CREATE INDEX IF NOT EXISTS idx_ejecucion_bpin ON ejecucion_presupuestal(bpin)",
            "CREATE INDEX IF NOT EXISTS idx_ejecucion_periodo ON ejecucion_presupuestal(periodo_corte)",
            
            # √çndices para equipamientos
            "CREATE INDEX IF NOT EXISTS idx_equipamientos_identificador ON unidades_proyecto_infraestructura_equipamientos(identificador)",
            "CREATE INDEX IF NOT EXISTS idx_equipamientos_comuna ON unidades_proyecto_infraestructura_equipamientos(comuna_corregimiento)",
            "CREATE INDEX IF NOT EXISTS idx_equipamientos_estado ON unidades_proyecto_infraestructura_equipamientos(estado_unidad_proyecto)",
            
            # √çndices para infraestructura vial
            "CREATE INDEX IF NOT EXISTS idx_vial_identificador ON unidades_proyecto_infraestructura_vial(identificador)",
            "CREATE INDEX IF NOT EXISTS idx_vial_comuna ON unidades_proyecto_infraestructura_vial(comuna_corregimiento)",
            "CREATE INDEX IF NOT EXISTS idx_vial_estado ON unidades_proyecto_infraestructura_vial(estado_unidad_proyecto)",
            
            # √çndices para seguimiento PA
            "CREATE INDEX IF NOT EXISTS idx_seguimiento_pa_periodo ON seguimiento_pa(periodo_corte)",
            "CREATE INDEX IF NOT EXISTS idx_seguimiento_pa_subdireccion ON seguimiento_pa(subdireccion_subsecretaria)",
            "CREATE INDEX IF NOT EXISTS idx_seguimiento_productos_pa_cod1 ON seguimiento_productos_pa(cod_pd_lvl_1)",
            "CREATE INDEX IF NOT EXISTS idx_seguimiento_productos_pa_cod2 ON seguimiento_productos_pa(cod_pd_lvl_2)",
            "CREATE INDEX IF NOT EXISTS idx_seguimiento_actividades_pa_cod1 ON seguimiento_actividades_pa(cod_pd_lvl_1)",
            "CREATE INDEX IF NOT EXISTS idx_seguimiento_actividades_pa_cod2 ON seguimiento_actividades_pa(cod_pd_lvl_2)",
            "CREATE INDEX IF NOT EXISTS idx_seguimiento_actividades_pa_cod3 ON seguimiento_actividades_pa(cod_pd_lvl_3)",
            
            # ‚úÖ √çndices para contratos SECOP
            "CREATE INDEX IF NOT EXISTS idx_contratos_bpin ON contratos(bpin)",
            "CREATE INDEX IF NOT EXISTS idx_contratos_cod_contrato ON contratos(cod_contrato)",
            "CREATE INDEX IF NOT EXISTS idx_contratos_estado ON contratos(estado_contrato)",
            "CREATE INDEX IF NOT EXISTS idx_contratos_proveedor ON contratos(codigo_proveedor)",
            "CREATE INDEX IF NOT EXISTS idx_contratos_valores_bpin ON contratos_valores(bpin)",
            "CREATE INDEX IF NOT EXISTS idx_contratos_valores_cod_contrato ON contratos_valores(cod_contrato)"
        ]
        
        try:
            with self.engine.connect() as connection:
                with connection.begin():
                    # Crear √≠ndices con barra de progreso
                    with tqdm(total=len(indices), desc="Creando √≠ndices", 
                             unit="√≠ndices", ncols=100) as pbar:
                        
                        for index_sql in indices:
                            try:
                                # Extraer nombre del √≠ndice para mostrar
                                index_name = index_sql.split('idx_')[1].split(' ')[0]
                                pbar.set_description(f"Creando √≠ndice {index_name}")
                                
                                connection.execute(text(index_sql))
                                logger.info(f"‚úÖ √çndice creado: {index_name}")
                                
                            except Exception as e:
                                if "already exists" in str(e):
                                    logger.info(f"‚ö†Ô∏è √çndice ya existe: {index_name}")
                                else:
                                    logger.warning(f"‚ö†Ô∏è Error creando √≠ndice {index_name}: {e}")
                            
                            pbar.update(1)
                    
                    logger.info(f"‚úÖ Procesamiento de √≠ndices completado ({len(indices)} √≠ndices)")
                    
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error durante creaci√≥n de √≠ndices: {e}")
    
    def fix_existing_schema_issues(self):
        """Corrige problemas comunes de tipos de columnas en tablas existentes con barra de progreso."""
        logger.info("üîÑ Verificando y corrigiendo esquemas existentes...")
        
        schema_fixes = [
            # Corregir tipos de columnas texto
            {
                'table': 'centros_gestores',
                'column': 'nombre_centro_gestor',
                'fix': "ALTER TABLE centros_gestores ALTER COLUMN nombre_centro_gestor TYPE TEXT"
            },
            {
                'table': 'programas',
                'column': 'nombre_programa', 
                'fix': "ALTER TABLE programas ALTER COLUMN nombre_programa TYPE TEXT"
            },
            {
                'table': 'areas_funcionales',
                'column': 'nombre_area_funcional',
                'fix': "ALTER TABLE areas_funcionales ALTER COLUMN nombre_area_funcional TYPE TEXT"
            },
            {
                'table': 'propositos',
                'column': 'nombre_proposito',
                'fix': "ALTER TABLE propositos ALTER COLUMN nombre_proposito TYPE TEXT"
            },
            {
                'table': 'retos',
                'column': 'nombre_reto',
                'fix': "ALTER TABLE retos ALTER COLUMN nombre_reto TYPE TEXT"
            },
            # Corregir tipos para movimientos y ejecuci√≥n
            {
                'table': 'movimientos_presupuestales',
                'column': 'bpin',
                'fix': "ALTER TABLE movimientos_presupuestales ALTER COLUMN bpin TYPE BIGINT"
            },
            {
                'table': 'movimientos_presupuestales',
                'column': 'periodo_corte',
                'fix': "ALTER TABLE movimientos_presupuestales ALTER COLUMN periodo_corte TYPE VARCHAR(50) USING periodo_corte::VARCHAR(50)"
            },
            {
                'table': 'ejecucion_presupuestal',
                'column': 'bpin',
                'fix': "ALTER TABLE ejecucion_presupuestal ALTER COLUMN bpin TYPE BIGINT"
            },
            {
                'table': 'ejecucion_presupuestal',
                'column': 'periodo_corte',
                'fix': "ALTER TABLE ejecucion_presupuestal ALTER COLUMN periodo_corte TYPE VARCHAR(50) USING periodo_corte::VARCHAR(50)"
            },
            # Corregir tipos para unidades de proyecto vial
            {
                'table': 'unidades_proyecto_infraestructura_vial',
                'column': 'identificador',
                'fix': "ALTER TABLE unidades_proyecto_infraestructura_vial ALTER COLUMN identificador TYPE VARCHAR(255) USING identificador::VARCHAR(255)"
            }
        ]
        
        try:
            with self.engine.connect() as connection:
                # Procesar correcciones con barra de progreso
                with tqdm(total=len(schema_fixes), desc="Corrigiendo esquemas", 
                         unit="columnas", ncols=100) as pbar:
                    
                    for fix in schema_fixes:
                        pbar.set_description(f"Corrigiendo {fix['table']}.{fix['column']}")
                        
                        try:
                            logger.info(f"üîß Corrigiendo {fix['table']}.{fix['column']}")
                            with connection.begin():
                                connection.execute(text(fix['fix']))
                            logger.info(f"‚úÖ {fix['table']}.{fix['column']} corregido")
                            
                        except ProgrammingError as e:
                            if "already exists" in str(e) or "does not exist" in str(e):
                                logger.info(f"‚ö†Ô∏è {fix['table']}.{fix['column']} ya est√° correcto o no existe")
                            else:
                                logger.warning(f"‚ö†Ô∏è No se pudo corregir {fix['table']}.{fix['column']}: {e}")
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Error menor corrigiendo {fix['table']}.{fix['column']}: {e}")
                        
                        pbar.update(1)
                
                logger.info(f"‚úÖ Correcciones de esquema completadas ({len(schema_fixes)} verificaciones)")
                        
        except Exception as e:
            logger.error(f"‚ùå Error durante correcci√≥n de esquemas: {e}")
    
    def verify_final_schema(self) -> bool:
        """Verifica que el esquema final est√© correcto consultando information_schema."""
        logger.info("üîç Verificando esquema final...")
        
        expected_tables = [
            'centros_gestores', 'programas', 'areas_funcionales',
            'propositos', 'retos', 'movimientos_presupuestales', 
            'ejecucion_presupuestal', 'datos_caracteristicos_proyectos',  # Nueva tabla
            'unidades_proyecto_infraestructura_equipamientos',
            'unidades_proyecto_infraestructura_vial', 'seguimiento_pa',
            'seguimiento_productos_pa', 'seguimiento_actividades_pa',
            'contratos', 'contratos_valores'  # ‚úÖ Nuevas tablas de contratos SECOP
        ]
        
        try:
            with self.engine.connect() as connection:
                # Verificar que todas las tablas existen
                existing_tables = self.get_existing_tables()
                missing_tables = [t for t in expected_tables if t not in existing_tables]
                
                if missing_tables:
                    logger.error(f"‚ùå Tablas faltantes: {missing_tables}")
                    return False
                
                # Verificar tipos de columnas cr√≠ticas
                schema_check = text("""
                    SELECT table_name, column_name, data_type, character_maximum_length
                    FROM information_schema.columns 
                    WHERE table_schema = 'public' 
                    AND table_name IN ('movimientos_presupuestales', 'ejecucion_presupuestal', 
                                     'unidades_proyecto_infraestructura_equipamientos',
                                     'unidades_proyecto_infraestructura_vial',
                                     'seguimiento_pa', 'seguimiento_productos_pa', 
                                     'seguimiento_actividades_pa', 'contratos', 'contratos_valores')
                    AND column_name IN ('bpin', 'periodo_corte', 'identificador', 'id_seguimiento_pa',
                                      'cod_pd_lvl_1', 'cod_pd_lvl_2', 'cod_pd_lvl_3', 'cod_contrato')
                    ORDER BY table_name, column_name;
                """)
                
                result = connection.execute(schema_check)
                schema_ok = True
                
                for row in result:
                    table_name, column_name, data_type, max_length = row
                    logger.info(f"üìä {table_name}.{column_name}: {data_type} ({max_length})")
                    
                    if column_name == 'bpin' and data_type != 'bigint':
                        logger.error(f"‚ùå {table_name}.bpin deber√≠a ser bigint, es {data_type}")
                        schema_ok = False
                    elif column_name == 'periodo_corte' and data_type != 'character varying':
                        logger.error(f"‚ùå {table_name}.periodo_corte deber√≠a ser varchar, es {data_type}")
                        schema_ok = False
                    elif column_name == 'identificador' and data_type != 'character varying':
                        logger.error(f"‚ùå {table_name}.identificador deber√≠a ser varchar, es {data_type}")
                        schema_ok = False
                    elif column_name == 'cod_contrato' and data_type != 'character varying':
                        logger.error(f"‚ùå {table_name}.cod_contrato deber√≠a ser varchar, es {data_type}")
                        schema_ok = False
                
                if schema_ok:
                    logger.info("‚úÖ Esquema verificado correctamente")
                    return True
                else:
                    logger.error("‚ùå Problemas encontrados en el esquema")
                    return False
                    
        except Exception as e:
            logger.error(f"‚ùå Error durante verificaci√≥n de esquema: {e}")
            return False
    
    def load_json_data_to_table(self, json_path: str, table_name: str) -> Tuple[bool, int]:
        """Carga datos desde un archivo JSON a una tabla espec√≠fica con verificaci√≥n incremental."""
        start_time = time.time()
        
        try:
            if not os.path.exists(json_path):
                logger.debug(f"üìÇ Archivo JSON no encontrado: {json_path}")
                return False, 0
            
            # Verificar si se debe cargar la tabla
            json_file_path = Path(json_path)
            if not self._should_load_data(table_name, json_file_path):
                return True, 0  # √âxito pero sin cargar datos nuevos
                
            # Registrar memoria antes de cargar
            memory_before = self._get_memory_usage()
            
            # Mostrar que estamos cargando el archivo
            file_size_mb = os.path.getsize(json_path) / (1024 * 1024)
            logger.info(f"üì• Cargando {os.path.basename(json_path)} ({file_size_mb:.2f} MB)")
            
            # Leer archivo JSON con barra de progreso
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if not data:
                logger.warning(f"üìÇ Archivo JSON vac√≠o: {json_path}")
                return False, 0
            
            if not isinstance(data, list):
                logger.error(f"‚ùå El archivo JSON debe contener una lista: {json_path}")
                return False, 0
                
            if len(data) == 0:
                logger.warning(f"üìÇ Lista vac√≠a en JSON: {json_path}")
                return False, 0
            
            total_records = len(data)
            logger.info(f"üìä Procesando {total_records:,} registros para tabla '{table_name}'")
            
            # Registrar m√©tricas del archivo
            self.metrics['files_processed'] += 1
            self.metrics['performance'][f'{table_name}_file_size_mb'] = file_size_mb
            self.metrics['performance'][f'{table_name}_total_records'] = total_records
            
            # Verificar estructura del primer registro
            sample = data[0]
            if not isinstance(sample, dict):
                logger.error(f"‚ùå Los registros deben ser diccionarios: {json_path}")
                return False, 0
                
            columns = list(sample.keys())
            logger.info(f"üìã Columnas detectadas: {len(columns)} ({', '.join(columns[:5])}{'...' if len(columns) > 5 else ''})")
            
            # Limpiar datos antes de insertar
            clean_data = self._clean_data_for_insert(data, table_name)
            
            # Limpiar tabla antes de cargar si tiene datos
            self._should_clean_table_before_load(table_name)
            
            # Preparar statement SQL
            placeholders = ', '.join([f':{col}' for col in columns])
            columns_str = ', '.join(columns)
            insert_sql = f"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders})"
            
            # Insertar datos en lotes con barra de progreso
            batch_size = 500  # Reducir tama√±o de lote para mejor manejo de errores
            successful_inserts = 0
            failed_inserts = 0
            
            with self.engine.begin() as connection:
                # Crear barra de progreso
                with tqdm(total=total_records, desc=f"Insertando en {table_name}", 
                         unit="registros", ncols=100) as pbar:
                    
                    for i in range(0, total_records, batch_size):
                        batch = clean_data[i:i + batch_size]
                        try:
                            # Insertar lote
                            connection.execute(text(insert_sql), batch)
                            successful_inserts += len(batch)
                            pbar.update(len(batch))
                            
                        except Exception as batch_error:
                            logger.warning(f"‚ö†Ô∏è Error en lote {i//batch_size + 1}: {batch_error}")
                            
                            # Intentar insertar registro por registro en este lote
                            for record in batch:
                                try:
                                    connection.execute(text(insert_sql), record)
                                    successful_inserts += 1
                                except Exception as record_error:
                                    failed_inserts += 1
                                    logger.debug(f"‚ùå Error en registro: {record_error}")
                                
                                pbar.update(1)
            
            # Reportar resultados
            if successful_inserts > 0:
                # Registrar m√©tricas
                end_time = time.time()
                load_time = end_time - start_time
                memory_after = self._get_memory_usage()
                
                self.metrics['data_loaded'][table_name] = successful_inserts
                self.metrics['total_records'] += successful_inserts
                self.metrics['failed_records'] += failed_inserts
                self.metrics['performance'][f'{table_name}_load_time_seconds'] = load_time
                self.metrics['performance'][f'{table_name}_records_per_second'] = successful_inserts / load_time if load_time > 0 else 0
                self.metrics['memory_usage'][f'{table_name}_memory_before_mb'] = memory_before['rss_mb']
                self.metrics['memory_usage'][f'{table_name}_memory_after_mb'] = memory_after['rss_mb']
                self.metrics['memory_usage'][f'{table_name}_memory_increase_mb'] = memory_after['rss_mb'] - memory_before['rss_mb']
                
                logger.info(f"‚úÖ {table_name}: {successful_inserts:,} registros cargados exitosamente")
                logger.info(f"‚è±Ô∏è {table_name}: Cargado en {load_time:.2f}s ({successful_inserts/load_time:.1f} reg/s)")
                if failed_inserts > 0:
                    logger.warning(f"‚ö†Ô∏è {table_name}: {failed_inserts:,} registros fallaron")
                    self.metrics['errors'].append({
                        'table': table_name,
                        'type': 'failed_records',
                        'count': failed_inserts,
                        'timestamp': datetime.datetime.now()
                    })
                return True, successful_inserts
            else:
                logger.error(f"‚ùå {table_name}: No se pudo cargar ning√∫n registro")
                self.metrics['errors'].append({
                    'table': table_name,
                    'type': 'complete_failure',
                    'message': 'No se pudo cargar ning√∫n registro',
                    'timestamp': datetime.datetime.now()
                })
                return False, 0
                    
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Error de formato JSON en {json_path}: {e}")
            return False, 0
        except Exception as e:
            logger.error(f"‚ùå Error cargando {json_path} a {table_name}: {e}")
            return False, 0

    def _clean_data_for_insert(self, data: list, table_name: str) -> list:
        """Limpia y prepara los datos para inserci√≥n, manejando valores NULL y tipos."""
        clean_data = []
        
        for record in data:
            cleaned_record = {}
            for key, value in record.items():
                # Manejar valores NULL, NaN, y strings vac√≠os
                if value is None or value == '' or (isinstance(value, str) and value.lower() in ['null', 'nan', 'none']):
                    cleaned_record[key] = None
                # Manejar n√∫meros como strings
                elif isinstance(value, str) and value.replace('.', '', 1).replace('-', '', 1).isdigit():
                    try:
                        # Intentar convertir a entero o float
                        if '.' in value:
                            cleaned_record[key] = float(value)
                        else:
                            cleaned_record[key] = int(value)
                    except ValueError:
                        cleaned_record[key] = value
                # Para fechas, asegurar formato correcto
                elif key in ['fecha_inicio_planeado', 'fecha_fin_planeado', 'fecha_inicio_real', 'fecha_fin_real', 'fecha_actualizacion']:
                    if value and isinstance(value, str) and value.strip():
                        cleaned_record[key] = value.strip()
                    else:
                        cleaned_record[key] = None
                # Para campos BPIN, asegurar que sea num√©rico
                elif key == 'bpin':
                    if value is None or value == '':
                        # Para Railway, mejor omitir el registro si BPIN es NULL
                        if self.is_railway_environment():
                            return clean_data  # Salir temprano, omitir este registro
                        cleaned_record[key] = None
                    else:
                        try:
                            cleaned_record[key] = int(float(str(value))) if value else None
                        except (ValueError, TypeError):
                            if self.is_railway_environment():
                                return clean_data  # Omitir registro con BPIN inv√°lido
                            cleaned_record[key] = None
                else:
                    cleaned_record[key] = value
            
            # Solo agregar si el registro no est√° vac√≠o
            if cleaned_record:
                clean_data.append(cleaned_record)
        
        return clean_data

    def _should_clean_table_before_load(self, table_name: str) -> bool:
        """Determina si una tabla debe limpiarse antes de cargar datos."""
        # Solo limpiar si no est√° vac√≠a
        with self.engine.connect() as conn:
            result = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
            count = result.scalar()
            if count > 0:
                logger.info(f"üßπ Tabla '{table_name}' limpiada antes de cargar")
                conn.execute(text(f"TRUNCATE TABLE {table_name} RESTART IDENTITY CASCADE"))
                conn.commit()
                return True
        return False
    
    def load_available_data(self) -> Dict[str, int]:
        """Busca y carga todos los datos disponibles desde archivos JSON con barras de progreso."""
        logger.info("üì¶ Iniciando b√∫squeda y carga de datos...")
        
        loaded_counts = {}
        
        # Mapeo de archivos a tablas
        file_table_mapping = {
            # Cat√°logos b√°sicos (JSON)
            "centros_gestores.json": "centros_gestores",
            "programas.json": "programas", 
            "areas_funcionales.json": "areas_funcionales",
            "propositos.json": "propositos",
            "retos.json": "retos",
            "proyectos.json": "proyectos",
            
            # Datos principales (nombres reales de archivos)
            "movimientos_presupuestales.json": "movimientos_presupuestales",
            "ejecucion_presupuestal.json": "ejecucion_presupuestal",
            "datos_caracteristicos_proyectos.json": "datos_caracteristicos_proyectos",
            "seguimiento_pa.json": "seguimiento_pa",
            "seguimiento_productos_pa.json": "seguimiento_productos_pa", 
            "seguimiento_actividades_pa.json": "seguimiento_actividades_pa",
            "unidad_proyecto_infraestructura_equipamientos.json": "unidades_proyecto_infraestructura_equipamientos",
            "unidad_proyecto_infraestructura_vial.json": "unidades_proyecto_infraestructura_vial",
            "contratos.json": "contratos",
            "contratos_valores.json": "contratos_valores"
        }
        
        # Buscar todos los archivos disponibles
        files_found = []
        
        # Buscar en directorios de datos
        for data_dir in self.data_directories:
            if not os.path.exists(data_dir):
                continue
                
            for file_name, table_name in file_table_mapping.items():
                json_path = os.path.join(data_dir, file_name)
                if os.path.exists(json_path):
                    files_found.append((json_path, table_name, file_name))
        
        # Tambi√©n buscar en directorio ra√≠z
        for file_name, table_name in file_table_mapping.items():
            if os.path.exists(file_name):
                files_found.append((file_name, table_name, file_name))
        
        if not files_found:
            logger.warning("‚ö†Ô∏è No se encontraron archivos de datos para cargar")
            return loaded_counts
        
        logger.info(f"üìã Encontrados {len(files_found)} archivos para procesar")
        
        # Procesar archivos con barra de progreso general
        with tqdm(total=len(files_found), desc="Procesando archivos", 
                 unit="archivos", ncols=100, position=0) as file_pbar:
            
            for json_path, table_name, file_name in files_found:
                file_pbar.set_description(f"Procesando {file_name}")
                
                # Limpiar tabla antes de cargar (opcional)
                try:
                    with self.engine.begin() as connection:
                        connection.execute(text(f"DELETE FROM {table_name}"))
                    logger.info(f"üßπ Tabla '{table_name}' limpiada antes de cargar")
                except Exception as e:
                    logger.debug(f"‚ö†Ô∏è No se pudo limpiar {table_name}: {e}")
                
                # Cargar datos
                success, count = self.load_json_data_to_table(json_path, table_name)
                
                if success and count > 0:
                    loaded_counts[table_name] = count
                    logger.info(f"‚úÖ {table_name}: {count:,} registros cargados correctamente")
                else:
                    logger.warning(f"‚ö†Ô∏è {table_name}: No se cargaron datos desde {file_name}")
                
                file_pbar.update(1)
        
        # Resumen final
        total_loaded = sum(loaded_counts.values())
        logger.info("=" * 60)
        logger.info(f"üéâ Carga completada: {len(loaded_counts)}/{len(files_found)} archivos procesados exitosamente")
        logger.info(f"üìä Total de registros cargados: {total_loaded:,}")
        
        if loaded_counts:
            logger.info("üìã Resumen por tabla:")
            for table, count in sorted(loaded_counts.items()):
                logger.info(f"   ‚Ä¢ {table}: {count:,} registros")
        
        return loaded_counts
    
    def verify_data_integrity(self, loaded_counts: Dict[str, int]) -> bool:
        """Verifica que los datos se hayan cargado correctamente."""
        logger.info("üîç Verificando integridad de datos...")
        
        try:
            with self.engine.connect() as connection:
                # Verificar totales por tabla
                total_records = 0
                tables_with_data = 0
                
                expected_tables = [
                    'centros_gestores', 'programas', 'areas_funcionales',
                    'propositos', 'retos', 'movimientos_presupuestales', 
                    'ejecucion_presupuestal', 'datos_caracteristicos_proyectos',
                    'unidades_proyecto_infraestructura_equipamientos',
                    'unidades_proyecto_infraestructura_vial', 'seguimiento_pa',
                    'seguimiento_productos_pa', 'seguimiento_actividades_pa',
                    'contratos', 'contratos_valores'
                ]
                
                logger.info("üìä Estado de datos por tabla:")
                for table in expected_tables:
                    try:
                        result = connection.execute(text(f"SELECT COUNT(*) FROM {table}"))
                        count = result.scalar()
                        total_records += count
                        
                        if count > 0:
                            tables_with_data += 1
                            logger.info(f"   ‚úÖ {table}: {count:,} registros")
                        else:
                            logger.warning(f"   ‚ö†Ô∏è {table}: Sin datos")
                            
                    except Exception as e:
                        logger.error(f"   ‚ùå {table}: Error - {e}")
                
                logger.info(f"üìä Resumen: {tables_with_data}/{len(expected_tables)} tablas con datos")
                logger.info(f"üìä Total de registros: {total_records:,}")
                
                # Verificaciones de integridad b√°sicas
                integrity_ok = True
                
                # Verificar que existan cat√°logos b√°sicos
                basic_catalogs = ['centros_gestores', 'programas', 'areas_funcionales', 'propositos', 'retos']
                for catalog in basic_catalogs:
                    result = connection.execute(text(f"SELECT COUNT(*) FROM {catalog}"))
                    count = result.scalar()
                    if count == 0:
                        logger.warning(f"‚ö†Ô∏è Cat√°logo b√°sico vac√≠o: {catalog}")
                        # No es cr√≠tico, solo advertencia
                
                # Verificar consistencia de claves for√°neas en proyectos
                if total_records > 1000:  # Solo si hay datos significativos
                    try:
                        # Verificar que los BPIN en movimientos existan en ejecuci√≥n si ambas tienen datos
                        result = connection.execute(text("""
                            SELECT COUNT(*) FROM movimientos_presupuestales m 
                            WHERE NOT EXISTS (
                                SELECT 1 FROM ejecucion_presupuestal e 
                                WHERE e.bpin = m.bpin
                            ) AND (SELECT COUNT(*) FROM movimientos_presupuestales) > 0 
                            AND (SELECT COUNT(*) FROM ejecucion_presupuestal) > 0
                        """))
                        orphan_movements = result.scalar()
                        
                        if orphan_movements > 0:
                            logger.warning(f"‚ö†Ô∏è {orphan_movements} movimientos sin ejecuci√≥n correspondiente")
                        else:
                            logger.info("‚úÖ Consistencia BPIN verificada")
                            
                    except Exception as e:
                        logger.debug(f"Verificaci√≥n de integridad opcional fall√≥: {e}")
                
                return integrity_ok
                
        except Exception as e:
            logger.error(f"‚ùå Error durante verificaci√≥n de integridad: {e}")
            return False
    
    def initialize_database(self) -> bool:
        """Orquesta el flujo completo de inicializaci√≥n, carga de datos y verificaci√≥n."""
        return self.complete_initialization()
    
    def complete_initialization(self) -> bool:
        """Ejecuta el proceso completo de inicializaci√≥n con m√©tricas."""
        try:
            logger.info("üöÄ Iniciando inicializaci√≥n completa de la base de datos")
            logger.info("=" * 70)
            
            # 1. Verificar conexi√≥n
            if not self.test_connection():
                return False
            
            # 2. Obtener estado actual
            existing_tables = self.get_existing_tables()
            backup_data = self.backup_existing_data(existing_tables)
            
            # 3. Crear/verificar estructura de tablas
            self.create_tables_with_correct_schema()
            
            # 4. Corregir problemas de esquema existentes
            self.fix_existing_schema_issues()
            
            # 5. Verificar esquema final
            if not self.verify_final_schema():
                logger.error("‚ùå La verificaci√≥n final del esquema fall√≥")
                return False
            
            # 6. Cargar datos disponibles
            logger.info("=" * 50)
            logger.info("üì¶ FASE DE CARGA DE DATOS")
            loaded_counts = self.load_available_data()
            
            # 7. Verificar integridad de datos
            if not self.verify_data_integrity(loaded_counts):
                logger.warning("‚ö†Ô∏è Algunas verificaciones de integridad fallaron, pero la estructura est√° lista")
            
            # 8. Reporte final
            logger.info("=" * 70)
            logger.info("üéâ INICIALIZACI√ìN COMPLETADA EXITOSAMENTE")
            logger.info("üìä Resumen final:")
            logger.info(f"   üåç Entorno: {self.metrics['environment']}")
            logger.info(f"   üóÑÔ∏è Tablas verificadas/creadas: {len(self.get_existing_tables())}")
            logger.info(f"   üì¶ Tablas con datos: {len(loaded_counts)}")
            
            total_loaded = sum(loaded_counts.values())
            logger.info(f"   üìä Total de registros: {total_loaded:,}")
            
            if loaded_counts:
                logger.info("   üìã Datos cargados por tabla:")
                for table, count in sorted(loaded_counts.items()):
                    logger.info(f"      ‚Ä¢ {table}: {count:,} registros")
            
            logger.info("   üöÄ Base de datos lista para el API")
            logger.info("=" * 70)
            
            # 9. Generar reporte detallado
            report_file = self.save_report()
            self.print_summary_report()
            
            if report_file:
                logger.info(f"üìÑ Reporte completo disponible en: {report_file}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error durante la inicializaci√≥n: {e}")
            self.metrics['errors'].append({
                'table': 'initialization',
                'type': 'fatal_error',
                'message': str(e),
                'timestamp': datetime.datetime.now()
            })
            return False
                logger.warning("‚ö†Ô∏è Algunas verificaciones de integridad fallaron, pero la estructura est√° lista")
            
            # 8. Reporte final
            logger.info("=" * 70)
            logger.info("üéâ INICIALIZACI√ìN COMPLETADA EXITOSAMENTE")
            logger.info("üìä Resumen final:")
            logger.info(f"   üåç Entorno: {'Railway (Producci√≥n)' if self.is_railway else 'Local (Desarrollo)'}")
            logger.info(f"   üóÑÔ∏è Tablas verificadas/creadas: {len(self.get_existing_tables())}")
            logger.info(f"   üì¶ Tablas con datos: {len(loaded_counts)}")
            
            total_loaded = sum(loaded_counts.values())
            logger.info(f"   üìä Total de registros: {total_loaded:,}")
            
            if loaded_counts:
                logger.info("   üìã Datos cargados por tabla:")
                for table, count in sorted(loaded_counts.items()):
                    logger.info(f"      ‚Ä¢ {table}: {count:,} registros")
            
            logger.info("   üöÄ Base de datos lista para el API")
            logger.info("=" * 70)
            
            # 9. Generar reporte detallado
            report_file = self.save_report()
            self.print_summary_report()
            
            if report_file:
                logger.info(f"üìÑ Reporte completo disponible en: {report_file}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error durante la inicializaci√≥n: {e}")
            self.metrics['errors'].append({
                'table': 'initialization',
                'type': 'fatal_error',
                'message': str(e),
                'timestamp': datetime.datetime.now()
            })
            return False
            
    def generate_markdown_report(self) -> str:
        """Genera un reporte completo en formato Markdown con m√©tricas y gr√°ficos."""
        end_time = datetime.datetime.now()
        total_duration = (end_time - self.metrics['start_time']).total_seconds()
        
        report = f"""# üìä Reporte de Inicializaci√≥n de Base de Datos
## API Dashboard Alcald√≠a de Cali

**Fecha de ejecuci√≥n:** {self.metrics['start_time'].strftime('%Y-%m-%d %H:%M:%S')}  
**Duraci√≥n total:** {total_duration:.2f} segundos  
**Entorno:** {self.metrics['environment']}  

---

## üìà Resumen Ejecutivo

| M√©trica | Valor |
|---------|-------|
| üóÑÔ∏è Tablas creadas/actualizadas | {self.metrics.get('tables_created', 0)} |
| üìÅ Archivos procesados | {self.metrics['files_processed']} |
| üìä Total de registros cargados | {self.metrics['total_records']:,} |
| ‚ùå Registros fallidos | {self.metrics['failed_records']:,} |
| ‚úÖ Tasa de √©xito | {((self.metrics['total_records']/(self.metrics['total_records']+self.metrics['failed_records']))*100) if (self.metrics['total_records']+self.metrics['failed_records']) > 0 else 100:.1f}% |

---

## üìã Detalle por Tabla

### ‚úÖ Datos Cargados
"""
        
        # Datos cargados
        if self.metrics['data_loaded']:
            for table, count in self.metrics['data_loaded'].items():
                load_time = self.metrics['performance'].get(f'{table}_load_time_seconds', 0)
                rate = self.metrics['performance'].get(f'{table}_records_per_second', 0)
                file_size = self.metrics['performance'].get(f'{table}_file_size_mb', 0)
                
                report += f"""
#### üìä {table}
- **Registros:** {count:,}
- **Tiempo de carga:** {load_time:.2f}s
- **Velocidad:** {rate:.1f} registros/segundo
- **Tama√±o del archivo:** {file_size:.2f} MB
"""
        else:
            report += "\n*No se cargaron datos nuevos.*\n"
            
        # Datos omitidos
        if self.metrics['data_skipped']:
            report += "\n### ‚è≠Ô∏è Datos Omitidos (ya existentes)\n"
            for table, count in self.metrics['data_skipped'].items():
                report += f"- **{table}:** {count:,} registros existentes\n"
        
        # Errores
        if self.metrics['errors']:
            report += f"\n### ‚ùå Errores Encontrados ({len(self.metrics['errors'])})\n"
            for error in self.metrics['errors']:
                report += f"""
#### {error['table']} - {error['type']}
- **Timestamp:** {error['timestamp'].strftime('%H:%M:%S')}
- **Detalles:** {error.get('message', error.get('count', 'N/A'))}
"""
        
        # M√©tricas de rendimiento
        report += f"""
---

## ‚ö° M√©tricas de Rendimiento

### üïê Tiempos de Procesamiento
"""
        
        if self.metrics['performance']:
            # Crear gr√°fico simple en texto de barras de tiempo
            time_metrics = {k.replace('_load_time_seconds', ''): v for k, v in self.metrics['performance'].items() if '_load_time_seconds' in k}
            if time_metrics:
                max_time = max(time_metrics.values())
                report += "\n```\nTiempos de carga por tabla:\n"
                for table, time_val in sorted(time_metrics.items(), key=lambda x: x[1], reverse=True):
                    bar_length = int((time_val / max_time) * 40) if max_time > 0 else 0
                    bar = "‚ñà" * bar_length + "‚ñë" * (40 - bar_length)
                    report += f"{table:<35} {bar} {time_val:6.2f}s\n"
                report += "```\n"
        
        # M√©tricas de memoria
        if self.metrics['memory_usage']:
            report += "\n### üíæ Uso de Memoria\n"
            memory_metrics = {}
            for k, v in self.metrics['memory_usage'].items():
                if '_memory_increase_mb' in k:
                    table = k.replace('_memory_increase_mb', '')
                    memory_metrics[table] = v
            
            if memory_metrics:
                total_memory_increase = sum(memory_metrics.values())
                report += f"\n**Aumento total de memoria:** {total_memory_increase:.2f} MB\n\n"
                report += "```\nAumento de memoria por tabla:\n"
                for table, mem_increase in sorted(memory_metrics.items(), key=lambda x: x[1], reverse=True):
                    report += f"{table:<35} {mem_increase:>8.2f} MB\n"
                report += "```\n"
        
        # Gr√°fico de distribuci√≥n de registros
        if self.metrics['data_loaded']:
            report += "\n### üìà Distribuci√≥n de Registros Cargados\n"
            total_loaded = sum(self.metrics['data_loaded'].values())
            max_count = max(self.metrics['data_loaded'].values())
            
            report += "\n```\nDistribuci√≥n por tabla:\n"
            for table, count in sorted(self.metrics['data_loaded'].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total_loaded) * 100 if total_loaded > 0 else 0
                bar_length = int((count / max_count) * 50) if max_count > 0 else 0
                bar = "‚ñà" * bar_length + "‚ñë" * (50 - bar_length)
                report += f"{table:<35} {bar} {count:>8,} ({percentage:5.1f}%)\n"
            report += "```\n"
        
        # Recomendaciones
        report += f"""
---

## üí° Recomendaciones

### üîß Optimizaciones Identificadas
"""
        
        recommendations = []
        
        # Verificar si hay tablas con muchos errores
        error_tables = {}
        for error in self.metrics['errors']:
            if error['type'] == 'failed_records':
                table = error['table']
                count = error.get('count', 0)
                error_tables[table] = error_tables.get(table, 0) + count
        
        for table, error_count in error_tables.items():
            total_attempts = self.metrics['data_loaded'].get(table, 0) + error_count
            if total_attempts > 0:
                error_rate = (error_count / total_attempts) * 100
                if error_rate > 10:
                    recommendations.append(f"- ‚ö†Ô∏è **{table}:** {error_rate:.1f}% de registros fallan - revisar calidad de datos")
        
        # Verificar rendimiento
        if self.metrics['performance']:
            slow_tables = []
            for k, v in self.metrics['performance'].items():
                if '_records_per_second' in k and v < 50:  # Menos de 50 registros por segundo
                    table = k.replace('_records_per_second', '')
                    slow_tables.append(f"- üêå **{table}:** {v:.1f} reg/s - considerar optimizaci√≥n de √≠ndices")
            recommendations.extend(slow_tables)
        
        if recommendations:
            report += "\n" + "\n".join(recommendations) + "\n"
        else:
            report += "\n- ‚úÖ **Rendimiento √≥ptimo:** No se detectaron problemas de rendimiento\n"
        
        report += f"""
### üìä Pr√≥ximos Pasos
- ‚úÖ Base de datos lista para consultas
- üöÄ Iniciar API: `uvicorn fastapi_project.main:app --reload`
- üìä Monitorear logs para consultas lentas
- üîÑ Programar respaldos incrementales

---

*Reporte generado autom√°ticamente el {end_time.strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        return report
        
    def save_report(self) -> str:
        """Guarda el reporte en un archivo Markdown."""
        report_content = self.generate_markdown_report()
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"database_initialization_report_{timestamp}.md"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(report_content)
            logger.info(f"üìÑ Reporte guardado en: {filename}")
            return filename
        except Exception as e:
            logger.error(f"‚ùå Error guardando reporte: {e}")
            return ""
            
    def print_summary_report(self):
        """Imprime un resumen del reporte en consola."""
        total_duration = (datetime.datetime.now() - self.metrics['start_time']).total_seconds()
        
        print("\n" + "="*80)
        print("üéâ RESUMEN DE INICIALIZACI√ìN COMPLETADA")
        print("="*80)
        print(f"‚è±Ô∏è Duraci√≥n total: {total_duration:.2f} segundos")
        print(f"üåç Entorno: {self.metrics['environment']}")
        print(f"üìÅ Archivos procesados: {self.metrics['files_processed']}")
        print(f"üìä Total registros cargados: {self.metrics['total_records']:,}")
        if self.metrics['failed_records'] > 0:
            print(f"‚ùå Registros fallidos: {self.metrics['failed_records']:,}")
        
        if self.metrics['data_loaded']:
            print(f"\nüìã Tablas con datos cargados ({len(self.metrics['data_loaded'])}):")
            for table, count in self.metrics['data_loaded'].items():
                print(f"   ‚Ä¢ {table}: {count:,} registros")
        
        if self.metrics['data_skipped']:
            print(f"\n‚è≠Ô∏è Tablas omitidas ({len(self.metrics['data_skipped'])}):")
            for table, count in self.metrics['data_skipped'].items():
                print(f"   ‚Ä¢ {table}: {count:,} registros existentes")
        
        print("="*80)

def main():
    """Funci√≥n principal para ejecutar la inicializaci√≥n completa"""
    print("üèõÔ∏è API Dashboard Alcald√≠a de Cali - Inicializador Unificado")
    print("üîß Estructura + Datos para entornos Locales y Railway")
    print("=" * 70)
    
    initializer = DatabaseInitializer()
    
    start_time = time.time()
    success = initializer.initialize_database()
    end_time = time.time()
    
    print(f"\n‚è±Ô∏è Tiempo total de ejecuci√≥n: {end_time - start_time:.2f} segundos")
    
    if success:
        print("‚úÖ Base de datos completamente configurada y lista para producci√≥n")
        print("üöÄ Puedes iniciar tu API con: uvicorn fastapi_project.main:app --reload")
        sys.exit(0)
    else:
        print("‚ùå Error en la inicializaci√≥n. Revisar logs para detalles.")
        print("üìã Log guardado en: database_init.log")
        sys.exit(1)

if __name__ == "__main__":
    main()
